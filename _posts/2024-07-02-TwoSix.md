---
layout: post 
title: Team TwoSix: Fine Tuning of Large Language Models (LLMs) to Generate Fuzzy Maps to Summarize Scientific Research
author: Ashlin Riggs, Spencer Dork, Krithi Sachithanand, Riley Millikan, Aadarsh Swaminathan
---

![TwoSix Fuzzy Graph Example](./images/TwoSix_Diagram.jpg)

In the spring of 2024 team [TwoSix](https://twosixtech.com/) of the Michigan State University Data Science Capstone is working to improve a software tool that analyzes passages from publications related to offshore fishing/wind turbines. The tool collects relational data to create a fuzzy map (see figure) that emphasizes causal relationships between “sources” and “targets” defined within the papers. These graphs can be used for many applications, such as performing simulations for scientific research or demonstrating how different factors in a system are related for policy making.
 
The Capstone Team is exploring Large Language Models (LLMs) as one tool that may expedite the process of extracting relational data to generate these types of fuzzy maps. LLMs are fine tuned for this application using a set of pre-labeled training data, allowing the LLM to automatically parse large corpus of scientific publications and extract relational concepts. Generating this labeled training data is currently a time-consuming, manual process of sifting through research papers to identify and label causal relationships. This project aims to simplify and expedite the data labeling process by creating a user interface (UI). 

To summarize, the two main tasks for our project are:
1.	Creating a data labeling UI so that humans may more easily generate JSON files containing source/target relationships from scientific passages
2.	Fine tuning existing LLMs using the UI-generated JSON files to automatically extract causal relationships from whole scientific publications

## Motivation
The motivation for this project is to simplify and expedite the data labeling process for causal relational data needed to generate fuzzy maps. These fuzzy maps may be used by scientists and policy makers to aid in their research and decision-making processes, respectively. Fuzzy maps demonstrate the complex interactions between several factors in a system, and they allow researchers to run simulations to study how increasing or decreasing one factor influences other related factors. This type of software has wide applications across a variety of scientific and political disciplines.

## User Interface
The first step of our project entails designing an intuitive UI for data labeling that minimizes unnecessary interactions while extracting important information from research papers or surveys. Our current focus is on creating a version of this UI that can be swiftly deployed, which will help increase the amount of available labeled data for training LLMs. To achieve this, we're leveraging the power of Dash in Python, which seamlessly integrates a backend for data handling with an accessible web-based frontend for efficient labeling. The interface streamlines the data labeling process, allowing researchers to effortlessly upload documents in various formats while quickly extracting important information. When a research paper is uploaded, the UI iterates through its sentences, allowing researchers to extract relevant data points as they label data. As for the data labeling itself, researchers can annotate data points with attributes like source, target, and direction in just a few clicks, providing essential context for analysis. Additionally, our interface outputs the labeled data in meticulously structured JSON files, which allows for seamless integration with our LLM training process. This approach not only streamlines the data labeling process but also enhances accessibility, allowing researchers to label data with ease and increasing the amount of data available for training the LLMs. By leveraging this labeled data, our LLM algorithms can more effectively identify relationships between research papers, enhancing the depth and quality of analysis.

## Fine Tuning
The fine tuning aspect of this project involves taking a pre-existing LLM and training it to more effectively extract relational information from scientific publications. This relational data can then be used to generate fuzzy maps for policy makers and researchers. We will train a fine tuning model on passages from scientific publications as input and the UI-generated JSON file relational data as the expected outputs. We will train only a small portion of LLM weights on this training data. After this, we will be “injecting” our newly re-trained weights into the LLM of our choice, which is the most important step of fine tuning a large language model for a new, specific task.

Our team is also actively engaged in exploring the various open-source LLMs available today and their specific use cases. This investigation involves delving into several notable LLMs, including ChatGPT, Falcon, and Llama, each offering unique features and potential strengths. Without a good LLM to use as a starting point, the fine tuning will not be nearly as successful at extracting necessary relational information. Key fine tuning techniques under our consideration currently include LoRA and Quantization.

The fine tuning process is computationally expensive, so using a personal device for completing most of the project will be difficult. As such, the group will use MSU’s High Performance Computing Center (HPCC) to complete a large part of the fine tuning aspect. The HPCC’s large availability of computational resources will help our models fine tune faster and more efficiently.

## Skills Learned
Through our efforts on this project, our team has had the opportunity to learn Dash for user interface generation, a variety of methods to perform large language model fine tuning, and how to implement fine tuning on MSU’s High Performance Computing Cluster. Without this project, our team never would have had these experiences during our time at Michigan State University. This project has given us the opportunity to flex our existing skills while continuing our learning.

## Thanks and Acknowledgements
The TwoSix team would like to thank our community partner representatives at TwoSix for their ongoing support this semester. The team would also like to thank our CMSE 495 instructors for their feedback and support. The strong leadership of our mentors is what will ultimately fuel our success this semester.
